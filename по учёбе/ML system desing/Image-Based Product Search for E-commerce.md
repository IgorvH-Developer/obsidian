Allow users to search for products using images instead of text. 
Product images, product descriptions, user search queries. 
Embeddings, two tower architectures, classic vs neural search techniques, colbert like models, cross-modal (image / text) models. 
Input: image, search for product page = image + text + comments


Поиск товаров по фото для онлайн-торговли
# Цели
Главная цель внедрения поиска товаров по фото - увеличить продажи товаров. 

Допустим, что есть онлайн-магазин, в котором уже имеется текстовый поиск. Внедрение новой модальности поиска (по фото) позволит пользователю точнее описывать свой запрос, а это даст возможность магазину предлагать товары, наиболее подходящие пользователю, что улучшит качество поиска. А улучшение качества поиска приводит к увеличению числа продаж товаров.

Качество поиска будет оцениваться различными online и offline метриками.

# Постановка задачи
Как уже было оговорено выше, имеется онлайн магазин с текстовым поиском, то есть уже имеется база данных с карточками товаров, представленных в магазине. 
Под карточкой товара подразумевается список из:
- фото
- название
- описание

**Тогда задача следующая**:
1) определение товаров на фото
2) поиск в БД карточек товаров, наиболее схожих с товарами, представленными на фото
**Входные данные**:
- Фото
**Результат**:
- Карточки товаров

## Постановка задачи в ML формате
1) Первый вариант - это создание текстового описания изображения.

2) Второй вариант - это создание векторного представления изображения (эмбеддинга).

К тому же, в каждом варианте можно добавить предварительную сегментацию изображения для выделения фрагментов, содержащих товары.

# Источники данных
## Данные для задачи сегментации
**Источники данных**:
1) Толокеры. Можно дать задание на создание датасета изображений с отмеченными фрагментами, содержащих товары. 
**Стоимость**:
1) Пусть привлечение толокеров для создания датасета будет стоить x_t.
## Данные для обработки изображений с товаром
**Источники данных**:
1) БД нашего магазина. Так как раннее было допущено, что уже существует БД с карточками товаров, то источником данных может стать эта БД.
2) Открытые источники. Можно использовать картинки с описанием из интернета. 
3) Сайты других онлайн-магазинов с карточками товаров.
**Стоимость создания датасета**:
1) Для задачи генерации текстового описания изображения стоимость составления датасета - беспалатно.
2) Для задачи генерации эмбеддинга изображения может потребоваться составить датасет из изображений и схожести товаров с изображением. Оценить схожесть изображений и товаров, например по шкале от 1 до 5, тоже могут толокеры. Стоимость - x_t.
# ML модель
Так как для работы с изображениями хорошо применимы модели машинного обучение, то без их использования нам не обойтись. Тем более, обработка изображений уже относительно хорошо изучена.
## Архитектура модели
### Байзлайн
В качестве байзлайна будет использоваться модель для генерации текстового описания изображения с дальнейшим текстовым поиском, как при обычном запросе.
Пример модели, генерирующей описание по изображению, от чата ГПТ:
1. CLIP (Contrastive Language–Image Pre-training)
   CLIP – это модель, разработанная OpenAI, которая была обучена на большом количестве пар «текст-изображение». Она может сопоставлять текстовые запросы с изображениями и наоборот. CLIP хорошо работает для классификации изображений и генерации текстовых описаний.
### Улучшения
#### Сегментация товаров на изображении
Перед генерацией описания можно предварительно выделить сегменты изображения, на которых содержится товар, и только потом генерировать описание по сегменту изображения.

Список популярных моделей сегментации объектов от чата ГПТ:
1. Mask R-CNN:
    - Эта модель является одной из наиболее популярных для детекции и сегментации объектов. Она сочетает в себе Region Proposal Network (RPN) и Fully Convolutional Network (FCN).
    - Mask R-CNN способна точно выделять контуры объектов на изображении, что делает ее отличным выбором для сегментации товаров.
2. U-Net:
    - U-Net – это популярная архитектура для сегментации медицинских изображений, но она также хорошо работает и для других типов изображений, включая фотографии товаров.
    - Модель имеет симметричную структуру энкодера-декодера, что позволяет ей эффективно извлекать контекстную информацию и восстанавливать пространственные особенности.
#### Генерация эмбеддингов
Вместо генерации текстового описания изображения, можно генерировать эмбеддинг изображения, так же с предварительной сегментацией. 
Данный шаг также изменит и архитектуру поисковика. Теперь первичный отбор карточек товаров будет производиться векторным поиском, то есть теперь точно потребуется содержать векторную БД.

Список моделей для генерации эмбеддингов изображений от чата ГПТ:
1. ResNet (Residual Networks):
    - Одна из самых известных архитектур глубоких нейронных сетей, которая используется для задач компьютерного зрения, включая классификацию изображений и создание эмбеддингов

Список моделей для генерации мультимодальных эмбеддингов от чата ГПТ:
1. CLIP (Contrastive Language–Image Pre-training)
	- CLIP — это модель, разработанная OpenAI, которая обучена сопоставлять текстовые описания с изображениями. Она состоит из двух частей: одна часть кодирует текст, другая — изображения. Модель обучалась на огромных наборах пар «текст-изображение» с использованием контрастивного обучения.
	- Применения: CLIP широко используется для различных задач, связанных с обработкой изображений и текстов, включая классификацию изображений, генерацию изображений по текстовым описаниям и многое другое.
2. DALL-E / DALL-E 2
	- Также разработана OpenAI.
	- Эти модели генерируют изображения на основе текстовых описаний.
	- В то время как DALL-E создает изображения на основе кодированных текстовых инструкций, DALL-E 2 улучшает качество изображений за счет использования диффузионных моделей.
	- Хотя основное назначение этих моделей – генерация изображений, они также могут использоваться для создания мультимодальных эмбеддингов.
#### Поиск с использованием ML
После первичного поиска товаров можно использовать ML модели для ранжирования и последующего отбора.
Для этого могут быть применимы следующие модели:
1) Би-энкодеры
   - Colbert
2) Кросс-энкодеры

Кросс-энкодеры слишком ресурсозатратные во время инференса, поэтому если их и использовать, то только для финальной стадии отбора товаров, когда осталось минимальное количество кандидатов.
А для второй стадии отбора (после получения K товаров из БД) можно использовать би-энкодер, а именно colbert.

**Замечание**. Возможно каждую из моделей выше придётся дообучать для более качественной работы со специфичными нам задачами. Для этого можно использовать датасеты, описанные в пукнте про данные.
# Оценка
## Оценка качества
Как-то нужно оценивать качество модели как отдельного компонента.

Также, качество модели можно оценивать по конечному результата поиска, а для этого подойдут метрики:
- offline метрики
  1) MAP@k
  2) MRR
  3) DSG
- online метрики
  1) CTR (click-through rate)
  2) first/last click time
# Развертывание системы
## Архитектура
![[5.png]]

## Потребляемые ресурсы
1) Генерация текста по изображению. Используем модель CLIP.
![[1.png]]
   ![[2.png]]
   
   Для байзлайна возьмём самую быструю версию модели, тогда инференс одного изображения займёт ~**16 GFLOPs**. Взято из [статьи](https://arxiv.org/pdf/2103.00020).
2) Сегментация изображений. Для примера рассмотрим модель Mask R-CNN.
   ![[3.png]]
   Для инференса одного изображения необходимо ~**0.5 GFLOPs**. Взято из [статьи](https://arxiv.org/pdf/2007.08921).
3) Генерация эмбеддинга изображения.
   Для генерации эмбеддинга снова используем CLIP и будем считать, что генерация эмбеддинга 1 изображения требует ~**16 GFLOPs**.
   И будем считать, что за 16 GFLOPs CLIP обрабатывает как цельное изображение, так и несколько его сегментов.
4) Ранжирование с использованием ML алгоритмов. Для этого используем colbert.
   ![[4.png]]
   Ранжирование 1000 объектов для одного запроса занимает **7 GFLOPs**. Взято из [статьи](https://arxiv.org/pdf/2004.12832).
**Вывод**.
Рассмотрим две конфигурации поиска товаров по изображению:
1) **Байзлайн**. Модель для генерации текста по изображению + текстовый поиск.
   Кол-во операций на изображение во время инференса: 16 GFLOPs + x_text
   Качество: будет оценено метриками выше
2) **Улучшенный вариант**. Сегментация изображения (Mask R-CNN) + генерация эмбеддинга (CLIP) + поиск по векторной БД + ранжирование (colbert)
   Кол-во операций на изображение во время инференса: 23.5 GFLOPs + x_vect
Допустим наш сервис предполагает 1000 запросов в секунду, тогда для инференса моделей:
1) Байзлайн. Необходимо железо, гарантирующее мощностью не менее 16 TFLOPs. Стоимость аренды видеокарт будет ~500$ в месяц (за две NVIDIA T4)
2) Улучшенный вариант. Необходимо железо, гарантирующее мощностью не менее 23.5 TFLOPs. Стоимость аренды видеокарт уже будет ~750$ в месяц (за три NVIDIA T4)
И это учитывая, что инференс всех моделей будет производиться на одном сервере.
Стоимости аренды видеокарт взяты с [сайта](https://cloud.google.com/compute/gpus-pricing).

Таким образом, чтобы внедрение поиска товаров по фото было целесообразным необходимо, чтобы этот функционал приносил как минимум дополнительные 500$ в месяц, чтобы покрыть затрачиваемые на него ресурсы (не считая затраты на дообучение моделей и зарплаты рабочих, они работают за еду).
А чтобы внедрять улучшенную версию поиска товаров по изображению, необходимо эта версия приносила уже как минимум 750$ в месяц.

А оценить сколько дохода приносит внедрённый функционал можно например по различным online метрикам по типу CTR и кол-во покупок в результате поиска.
