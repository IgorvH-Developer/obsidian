$L_1$ ранжирование. 
Это самый простой алгоритм ранжирования, который позволят получить предварительный топ документов.


### TF-IDF.
**Напоминание** tf - это количество термина в документе.
term saturation - это чтобы не слишком сильно колебать оценку. Чаще всего сглаживается как: $t' = \frac{tf}{tf+k}$
Если мы хотим изменить жёсткость, то $\frac{tf + \alpha k}{tf + k}$. Это вероятность, что случайно выбранный term оказался в документе.

Проблемы с подобными оценками в том, что для больших документов $tf$ всегда больше, чем для маленьких документов.
Чтобы решить эту проблему предлагается: $tf'' = \frac{tf}{tf + k * \frac{dl}{avg dl}}$
где $dl$ - длина документа, $avg dl$ - средняя длина в коллекции.

$tf''' = \frac{tf}{tf + k * (1 - b + b \frac{fl}{avg dl})}$ , $b \in [0;1]$ 

**Напоминание**. $IDF = log \frac{N}{df}$


### BM25
$BM25 = \Sigma_{t \in q} log(\frac{N}{df}) tf'''$


### Вероятностное ранжирование
$P(R=1 | d, q) > P(R=Q| d,q)$
где $R$ - релевантность.

Выражение слева это то, к чему мы стремимся.

$\frac{P(R=1 | d,q)}{P(R=1|d,q)} = \frac{P(d|R=1,q)P(R=1|q)}{P(d|q)} * \frac{P(d|q)}{P(D|R=0) ещё чёт} \sim \frac{P(d|R=1,q)}{P(d|R=0,q)} \sim = \frac{T_{t \in D}(P(t|q,R=1))}{...}$


Для каждого терма можно посчитать вероятность из таблички на холсте.
$\frac{P(d|q,R=1)}{P(d|q,R=0)} \sim \frac{\Pi_{t \notin d} (1 - p_t)} {\Pi_{t \notin d} (1 - u_t)}*\frac{\Pi_{t \notin d} (p_t)} {\Pi_{t \notin d} (u_t)}$

Вероятностное ранжирование $\to$ эмпирическая модель.

### Language modeling
$P(d|q)$


### Ещё что-то
Есть проблема, что никак не учитывается позиция терминов.
$L_1(q, d) = B25(q,d) * w_1 + Proximity(q,d)*w_2$
$proximity$ - близость

$Proximity(q,d) = \Sigma_{t_i,t_j \in Q} w_d(t_i, t_j) min\{tfidf, tfidf^2\}$

$tp(t_i, t_j) = \frac{1}{d(t_i, t_j)^2}$ - uniq per position
$w_d(t_i, t_j) = \frac{\Sigma_{pairs}tp(t_i, t_j)}{k + \Sigma_p tp(t_i, t_i)}$



$HHproximity$







