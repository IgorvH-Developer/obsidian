В 50-60 годах появился TREC (text retrival evaluation conference)

Оценка качества поиска оценивает SERP (search engine result page).

У нас есть запрос и выданные документы:
$q \to d_1, d_2,...,d_n$
где релевантные документы обозначаем 1, а нерелевантные - 0.

Первые метрики были:
**Presicion@K**
**Recall@K**
но это метрики не отображают позицию документа в выдаче, поэтому придумали метрика:
**MAP@K**

**Про выдачу документов**.
Для оценки релевантности так можно ориентироваться на:
1) Поймёт ли пользователь документ (на основе прошлого поиска)
2) Содержание
3) Форма
4) Профиль (то, в каком виде документ попадает в индекс)
**Про запросы пользователя**:
1) Истинная информационная потребность (ИП)
2) Осознанная ИП
3) Сформулированная ИП
4) Запрос

**Пертинентность** про связь истинной ИП и про то поймёт ли пользователь документ (первый пункт с первым).
**Релевантность** про то как запрос пользователя соотносится с тем как мы преподносим документ (четвёртый пункт с четвёртым)

Есть разные типы документов в выдаче:
- vital (например вк, ютуб)
- полезный (отвечает на запрос пользователя, полезный пользователю в целом)
- rel+
- rel-
- irrel

Запросы:
- навигационные
- информационные
- коммерческие
- новостные

Доп задачи:
- классификация запроса + интенты
- оценка качества документов / спам
- разбор запроса (запрос $\to$ осознанная ИП)

Список по увеличению качества оценки:
- crowdsourcing
- aceccment (профессиональные оценщики)
- эксперты (обычно возглавляют команды aceccment или пишут инструкторы)


Список конференций:
- TREC
- CLEF
- NTCTR
- РОМИП

- MS MARCO
- LOTTE
- Anserini