# Ранжирование. $L_0, L_1$
## Сборка индекса
Если документов становится слишком много, то вся схема засовывается в map reducer.
Если шардирование делать независимо, то множество идентификаторов в разных шардах может пересекаться. Поэтому хорошо, чтобы у каждого документа был свой идентификатор. Тогда у кадого постинга(?) будет храниться маппинг из (doc_id - item_id).

Это можно сказать база данных, а любая база данных поддерживает команды insert, update, delete.
**Insert**. Создаём новый шард.
**Delete**. Если мы документ удаляем, то в обратном индексе появляется запись (в кишке), что этот документ удалён. Эффективнее будет рядом хранить файл tombstone.
**Update**. Аптдейт это делит и инсерт.

Можно сказать каждый шард это независимое множество документов со своими индексами. Получается, что у нас несколько независимых индексов, и нам придётся смотреть все из них и мы не можем регулировать нагрузку на эти машины (которые хранят шарды). И было бы хорошо распределять индексы в зависимости от частоты индекса, чтобы на частотные запросы больше мощностей, а на менее частотные менее мощные.

Поэтому рассмотрим **term-sharded**.
По сути это процесс сборки базы. Мы строим кишки, где термины (стоящие в роли индексов) зависят от частоты запросов, использующихся в поисковике.

## $L_1$
Когда мы хотим поискать мы хотим ограничить множество документов 

Формула (1), записанная в тетрадке, не учитывает как этот кворум удовлетворяется.

**Формула (2)** векторный поиск:
$$L_1(d,q) = \Sigma_{term \in query(запрос)}tf(term, doc)*idf(term)$$
формула основана на tf, idf.

Предположения на котором это строится:
Для этой формулы появляются понятия:
- свободный запрос - содержит хотя бы одно слово из запроса
- запрос (документ) - мешок слов.
Частота слов в документе как-то пропорциональна тому, о чём документ.

tf(term, doc) - term frequncy. Сколько раз term встречается в doc.
$tf(term) = log|D|/|d_i termed, d_i \in D|$. То есть чем реже этот термин встречается во всех документах, тем больше значение функции.

idf(term) - А какие слова запроса будут чаще всего встречаться в документе

Различные варианты tf(term, doc):
- $tf' = tf / max (tf_{doc})$
- 

Проблема написанного выше, что мы предполагаем, что проходим кишку до конца. В реальных же поисковиках есть ограничения у stream (doc id) есть ограничения, например остановка после просмотренных 200k.
В случае такого большого количества документов 

Поэтому появляются:
**Глобальный порядок**.
Замечание. Кишки должны быть равнимыми.
1) g(doc) - ?
где doc - doc_id

Мы берём и оцениваем как-то документ (например у рекламы оценка это покупаемость товара или это может быть цена). Это порядок который даёт id документам (раньше мы устанавливали их случайно).

Чем плох глобальный порядок? Тем что он глобальный.

2) g(doc, term)
**Чемпионские списки**.
Описан в тетрадке


## Заметки
Кворум - минимальное число слов из запроса, необходимых в документе, чтобы этот документ нас удовлетворил. (состоит из булевых операторов).

item_id - это глобальный индендификатор документа.
doc_id - идентификатор документа в кишке.