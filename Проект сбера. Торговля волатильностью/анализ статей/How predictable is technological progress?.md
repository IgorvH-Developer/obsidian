[Сслыка](https://www.sciencedirect.com/science/article/pii/S0048733315001699).

## Ключевые моменты
### В статье используется подход мотивированный Moore’s Law.
Мур предсказал, что количество транзисторов в интегральных схемах будет удваиваться каждые два года. А это влияет например на увеличение вычислительных мощностей, уменьшение энергопотребления. И закон Мура может применяться шире, например в прогнозе на удвоение скорости вычислений каждые 18 месяцев.
### Они переформулировали формула закона Мура как геометрическое случайное блуждание с дрейфом.
**Закон Мура**:
$$p_t = p_0e^{\mu t}$$
, где $\mu$ - **скорость изменения цены технологии** 
Если технология улучшается, то $\mu < 0$.

Мы предполагаем, что **цены** все технологии следуют **геометрическому случайному блужданию**, т.е. зашумленная версия закона Мура:
$$y_{jt} = y_{j,(t-1)} + \mu_j + \eta_{jt}$$
где $\eta_jt \sim N(0, K_j^2)$, $\mu_j$ - скорость изменения технологии j.
 
**Дельта цен**:
$$eps = \tau (\mu - \overline{\mu}) + \Sigma_{i=t+1}^{t+\tau}\eta_i$$

Поскольку мы хотим агрегировать ошибки прогноза для технологий   
с различной изменчивостью. Тогда **функция ошибки**:  
Предполагая , что m > 3 это

$$\Xi(\tau) = E [ (\frac{eps}{\overline{K}})^2] = \frac{m-1}{m-3}(\tau + \frac{\tau^2}{m})$$
**Универсальная формула ошибки** (независящая от оценки дрифта, дисперсии и горизонта прогноза, то есть независима от параметров конкретной технологии и прогноза):
$$\epsilon = \frac{1}{\sqrt{A}}(\frac{eps}{\overline{K}}) \sim t(m-1))$$
, где t(m-1) - распределение стьюдента, $A = \tau +\frac{\tau^2}{m}$.

Теперь формулу выше нужно **обобщить для автокорреляции** (видимо, чтобы учитывать корреляцию между переменными). Поэтому результаты расширяются для ARIMA(0,1,1), 0 означает, что не используется авторегресионная часть и **получаем IMA(1,1)** вида:
$$y_y - y_{t-1} = \mu + v_t + \theta v_{t-1}$$
, где $v_t \sim N(0, \sigma^2)$.
Эта модель тоже - случайное геометрическое блуждание, но с корреляционным слагаемым, когда $\theta \neq 0$ ($\theta > 0$ - автокореляия положительная).

Ключевой величиной для объединения данных является **дисперсия**, по   
аналогии с предыдущей моделью называем K для этой модели . 
$$K^2 = var(y_t - y_{t-1}) = var(v_t + \theta v_{t-1})=(1 + \theta^2)\sigma^2$$


А если известная дисперсия, то **распределение ошибки предсказания**:
$$eps \sim N(0, \sigma^2 A^*)$$
, $A^* = -2\theta + (1 + \frac{2(m-1)\theta}{m} + \theta^2)(\tau + \frac{\tau^2}{m})$
Уравнение для $A$ выше - это $A^*$ при $\theta = 0$.

Когда необходимо оценить дисперсию, мы выводим оценку роста и распределения ошибок прогноза, предполагая, что $\overline{K}$ и $eps$ независимы . Тогда **мат ожид от среднеквадратичной нормализованной ошибки** равно:
$$\Xi = E [ (\frac{eps}{\overline{K}})^2] = \frac{m-1}{m-3} \frac{A^*}{1 + \theta^2}$$
Тогда **распределение масштибированного нормализованного прогноза ошибки**:
$$\epsilon = \frac{1}{\sqrt{A^* / (1 + \theta^2)}}(\frac{eps}{\overline{K}}) \sim t(m-1))$$
**Все эти формулы - только оценки**.

Уравнение выше можно рассматривать как распределение ошибок вокруг точечного прогноза, что позволяет объединить множество технологий в одно распределение. Это свойство чрезвычайно полезно для статистического тестирования, т.е. для определения качества модели. 
Но его наибольшая польза, как мы продемонстрируем в разделе 6, заключается в том, что он позволяет сформулировать прогноз распределения будущих затрат на данную технологию .

Использую некоторые из формулы выше можно представить **распределение предсказания будущего логарифма стоимости** при условии, что:
$(y_t, ..., y_{t-m+1})$ это $y_{t+r} \sim N(y_t + \overline{\mu}\tau, \overline{K}^2 \frac{A^*}{1 + \theta^2})$



