# Основные аспекты математической статистики

## Постановка задачи

Главная задача математической статистики заключается в оценке распределения случайных величин, основываясь на выборке данных.
Мы рассматриваем выборку$x_1, x_2, \ldots, x_n$, которая является реализацией случайных величин $\xi_1, \xi_2, \ldots, \xi_n$. И мы предполагаем, что случайные величины одинаково распределены и в совокупности независимы.
Наша цель — определить, каков закон распределения этих случайных величин, т.е. оценить функцию распределения $F_{\xi_i}$ и ее параметры.

Функцию распределения случайной величины можно обозначить как:
$F_\xi \in \{ F_\theta \}, \theta \in \Theta \subset R^m$
где $\theta$ - это параметр/параметры этого распределения.
## Инструменты для решения поставленной задачи
Приведу сначала список, а потом рассмотрю некоторые пункты подробнее:
1) Оценки параметров
   - Точечные оценки параметров
   - Методы построения точечных оценок
   - Интервальные оценки параметров
2) Эмпирическая функция распределения
3) Проверка статистических гипотез
### Точечные оценки параметров
**Определение**. Точечная оценка — это функция от элементов выборки, которая принимает значения в параметрическом пространстве $\Theta$ и обозначается как $\overline{\theta}_n$.
**Качество оценок**. 
Оценка может быть **несмещенной** (в среднем равна истинному значению параметра), если для $\overline{\theta}_n$ выполнено:
$E\overline{\theta}_n = \theta$
Может быть **состоятельной** (сходимость по вероятности оценочной функции к истинному значению при увеличении объема выборки), если выполнено:
$\overline{\theta}_n \to^P \theta$  при $n \to \infty$
Сходимость по вероятности это: $\forall \varepsilon > 0 \space P(|\overline{\theta}_n - \theta| > \varepsilon) \to 0, при \space n \to \infty$
И может быть **асимптотически нормальной**, если для $\overline{\theta}_n$  выполняется: $\sqrt{n} (\overline{\theta}_n - \theta) \to_{n \to \infty} \eta \sim N(0, \sigma^2(\theta))$ 
где $\sigma^2(\theta)$ - асимптотическая дисперсия.
### Методы построения точечных оценок
**Метод моментов**. 
Оценка параметров через равенство теоретических и эмпирических моментов. 
На нём я останавливаться не буду, скажу лишь, что для этого метода нам нужны априорные знания о моментах случайных величин до порядка k, где k - это количество параметров истинного распределения.

**Метод максимального правдоподобия**. 
Он в каком-то смысле более мощный. Здесь, в отличие от метода моментов, никакой догадки, априорных знаний нам не нужно.

По жизни (с которыми обычно работаем) распределения бывают двух типов: дискретные и абсолютно непрерывные. Поэтому введём в функцию, которую можно назвать плотностью для обоих типов распределения:
$f(x;\theta):=$ 
   1) $P_\theta(\xi=x) \space if \space дискретное$ 
   2) $p_\xi(x; \theta), \space if \space абс \space непрерыв$
И введём функция правдоподобия:
$L(x_1, ..., x_n, \theta)= \Pi_{i=1}^nf(x_i,\theta)$
Смысл функции такой: мы либо перемножаем в дискретном случае вероятности, либо перемножаем плотности с условиями $\xi_i = x_i$. То есть это просто либо вероятность того, что $\xi_1=x_1$ и так далее $\xi_n=x_n$, либо совместная плотность для $p_{\xi_1}(x_1, \theta)$ и так далее до $p_{\xi_n}(x_n, \theta)$.

В двух словах о методе. Можно сказать, что эта совместная вероятность (функция правдоподобия) показывает следующее. Чем больше значение этой функции, тем больше вероятность того, что именно такая случ величина $\xi_i$ принимает такое значение $x_i$ и так далее, при выбранных параметрах. И следовательно, тем правдоподобнее эти параметры, то есть тем вероятнее, что именно с этим параметром и задана функция распределения, которую используют высшие силы для описания нашей случайной величины.

Соответственно, нам нужна точка $\overline{\theta}_n$ максимума $L$.
Для нахождения максимума удобнее всего через экстремумы функции, то есть через производную и приравнивание к нулю. Но дифференцировать L тяжело, поэтому лучше взять логарифм от L. Точка максимума будет одна и та же, так как логарифм монотонная функция.
Следовательно, решив уравнение:
$\frac{\partial LnL}{\partial\theta} = 0$
мы найдём экстремум, а значит и нужную нам оценку $\overline{\theta}_n$.
### Эмпирическая функция распределения
Оценка функции распределения по данным выборки. Эмпирическая функция распределения имеет вид: 
$\overline{F}_n(x) = \frac{1}{n} \sum_{i=1}^n I_{\{\xi_i \leq x\}}$
Данная функция является несмещенной и состоятельной оценкой истинной функции распределения для каждого $x$. Но проблема в том, что мы смотрим только на точечные оценки функции распределения. А теорема ниже утверждает, что эта оценка также хорошо апроксимирует функцию распределения на всей прямой сразу.

Теорема Гливенко-Кантелли:
Если мы возьмём вероятность того, что супремум по всем x из R модуля разности оценки функции распределения и истинной функции распределения при $n \to \infty$  стремится к нулю, то это вероятность будет равна 1.
$$ P(sup_{x \in R} |\overline{F}_n(x, \xi_1,...,\xi_n) - F_{\xi_i}(x)| \to_{n \to \infty} 0) = 1 $$
То есть теорема утверждает, что для почти любой выборки, которая может породиться в рамках эксперимента, не просто разность между оценкой и истинным распределением близка к нулю, а даже супремум этой разности на все прямой стремиться к нулю при увеличении выборки. 

### Проверка статистических гипотез
Поиск доказательства или опровержения предварительных предположений о распределении данных или о параметрах распределения. 
Основные виды ошибок: ошибка 1-го рода (отказ от нашей гипотезы, хотя она верна) и ошибка 2-го рода (принятия нашей гипотезы, хотя она ложна). Ошибка 2-го рода самая страшная.

Допустим наша гипотеза в том, что выборочная функция распределения совпадает с известной нам функцией распределения, например с нормальной $N(0,1)$.
Как построить оценку функции распределения мы поняли, а так же убедились, что наша э.ф.р. апроксимирует истинную функцию распределения. 
А как узнать совпадает ли э.ф.р. с предполагаемой нами функцией распределения с такими-то вот параметрами? И с какой вероятностью мы можем ошибаться?
Отвечает $\cancel{Александр \space Друзь}$ Колмогоров.

Распишу это на примере критерия, который назван в честь Колмогорова.

Но сначала про **теорему Колмогорова**. Она утверждает, что выражение:
$\sqrt{n}D_n = \sqrt{n} \space sup_{x \in R}|\overline{F}_n(x, \xi_1, ..., \xi_n) - F_{\xi_i}(x)|$
можно привести к распределению, которое называется (сюрприз) Колмогоровским.

Связь теоремы Колмогорова и теоремы Гливенко-Кантелли можно сравнить со связью центральной предельной теоремы и закона больших чисел, формулы которых следующие:
$\frac{\xi_1 + ... + \xi_n - na}{n} \to 0$ - ЗБЧ
$\frac{\xi_1 + ... + \xi_n - na} {\sqrt{n}\sigma} \to^d N(0, 1)$ - ЦПТ

Как выражение из закона больших чисел можно преобразовать в случайную величину со стандартным нормальным распределением, так и преобразованное выражение из теоремы Гливенко-Кантелли подобным образом сходится по распределению к распределению Колмогорова.
И так, суть теоремы:
$D_n(\xi_i,...,\xi_n) = sup_{x \in R} |\overline{F}_n(x, \xi_1,...,\xi_n) - F_{\xi_i}(x)| \to_{n \to \infty} 0$ -  это выражение из теор. Г-К обозначим как $D_n$
И при условии, что $F_{\xi_i}(x)$ - непрерывно (это важно):
$\sqrt{n}D_n \to_{n \to \infty}^d \eta$
где $\eta$ - случайная величина с тем самым колмогоровским распределением:
$F_\eta(y) = K(y)$ 
1) $\space 0 \space при \space y \leq 0, \space$ 
2) $\space \Sigma_{i=-\infty}^\infty(-1)^i e^{-2i^2y^2} \space при \space y > 0$

Зачем нам вообще это надо, зачем приводить выражение $D_n$ к случайной величине с колмогоровским распределением?
Сначала стоит вспомнить для чего мы здесь, что мы хотим от нашей выборки элементов. А хотим мы понять как распределены эти числа, если вообще как-то распределены, по какому закону высшие силы спускают нам эти значения.
И связка теорем Гливенко-Кантелли и Колмогорова один из способов это сделать. Он называется критерием колмогорова.

**Критерий колмогорова** используется для проверки гипотез на согласованность выборки с какой-то известной функцией распределения. Делается это так, мы можем предположить, что в выражении $D_n = sup_{x \in R} |\overline{F}_n(x, \xi_1,...,\xi_n) - F_{\xi_i}(x)|$ истинная функция распределения $F_{\xi_i}(x)$ - это например ф.р. стандартного нормального распределения $N(0,1)$. И преобразуем его так:
$\sqrt{n}D_n = \sqrt{n}*sup_{x \in R}|\overline{F}_n(x, \xi_1, ..., \xi_n) - F_{\xi_i}(x)|$
Тогда логично предположить, что чем больше $\sqrt{n}D_n$ тем меньше наша э.ф.р. похожа на нормальную ф.р.. 
Но в математике любят, когда всё формализовано, поэтому отойдём чуть-чуть в сторону и введём понятие критического множества. Это такое множество $\Omega \subset R^n$, что если $(x_1,...,x_n) \in \Omega$ (выборка в него попадает), то отвергаем гипотезу.
Вернёмся к $\sqrt{n}D_n$ Мы ввели критическое множество, чтобы формализовать проверку насколько наше значение велико, то есть какой порог $c$ должно переступить значение $\sqrt{n}D_n$, чтобы считать, что оно слишком большое, то есть наша э.ф.р. слишком не похожа на гипотетическую ф.р..

Тогда критическое множество можно записать так:
$\Omega_C = \{(x_1,...,x_n): \space  \sqrt{n}D_n(x_1,...,x_n) > c \}$
Получается от $c$ зависит порог принятия решения (когда отвергаем гипотезу). И в зависимости от того как мы выберем эту константу будет зависеть вероятность ошибки 1-го рода (отвергаем нашу гипотезу, хотя она верна). Распишем это:
$P_0((\xi_1,...,\xi_n) \in \Omega) \sim P(\eta > c)$, что равно $= 1 - K(c)$
Эта точка:
$c_{1-\alpha}: \space K(c_{1-\alpha}) = 1 - \alpha$ 
будет $1 - \alpha$ квантиль колмогоровского распределения.
И если мы так выбираем c, то $\Omega$ переименуем в:
$\Omega_{\alpha} = \{(x_1,...,x_n): \space  \sqrt{n}D_n(x_1,...,x_n) > c_{1-\alpha} \}$
И эта точка $c_{1-\alpha}$ - критическая точка для $\alpha$ или же квантиль для $1 - \alpha$.

**Вывод**. 
Гипотеза тем разумнее (сильнее), чем выше для неё значение порога $\alpha$. 
То есть чем больше $\alpha$, тем меньше $c_{1-\alpha}$. Чем меньше $c_{1-\alpha}$, тем больше критическое множество $\Omega_{\alpha}$. Чем больше крит. множество $\Omega_{\alpha}$ и при этом наша гипотеза не отвергается, тем сильнее эта гипотеза.

И пару слов про **p-value**.
Часто при принятии решения по гипотезе сравниваются не критические точки ($\sqrt{n}D_n$ и $c_{1-\alpha}$), а p-value и $\alpha$. 
Но по сути это то же самое, то есть как $c_{1-\alpha}$ является критической точкой для $\alpha$, так и $\sqrt{n}D_n$ является критической точкой для p-value. 
Соответственно, если $\sqrt{n}D_n > c_{1-\alpha}$, то $p-value < \alpha$

По такой же логике выбора уровня значимости $\alpha$, критического множества $\Omega_{\alpha}$  и проверке гипотез работают и другие критерии, связанные как с проверкой выборок, так и с проверкой параметров выборок.