Ссылка на [лекции](https://ya.ru/video/preview/13173198930747210611).
## Постановка задачи математической статистики
Пусть у нас есть **выборка** (это набор действительных чисел): $x_1...x_n$.

Мы делаем допущение, что каждое из этих чисел являются реализацией какой-то случайной величины. Тогда имеем **случайные величины** $\xi_1...\xi_n$. Являющиеся отображениями $\Omega -> R$. Где $\Omega$ представляет из себя множество всех подмножеств элементарных событий.
**Например**: элементарные события $O =\{ w_1, w_2 \}$, где $w_1$ - орёл, $w_2$ - решка. И мы бросаем монетку 2 раза. Тогда $\Omega = \{ \{w_1, w_1\}, \{w_1, w_2\}, \{w_2, w_1\}, \{w_2, w_2\} \}$, мощность множества $|\Omega| = 2^{|O|}$. И случайная величина является отображение какого-то случайного события на множество вещественных чисел.

**Имеем**:
$\xi_1(w) = x1 ... \xi_n(w) = x_n$, где $\xi_i$ в совокупности **независимы и одинаково распределены**.

Чего мы не знаем, так это распределения этих случайных величин. **Задачей статистики и является оценка распределения этих случайных величин**.
## Точечные оценки параметров
Функция распределения случайной величины можно обозначить как:
$$ F_\xi \in \{ F_\theta \}, \theta \in \Theta \subset R^m $$
Где $F$ это распределение, а $\theta$ это параметр (параметры) этого распределения. **Например** если мы знаем, что распределение нормальное, тогда параметр $\theta = (\theta_1, \theta_2^2)$ что представляет из себя среднее и дисперсию, которые и задают нормальное распределение.
### Оценка (точечная) параметров 
**Опр**. *Это любая функция от элементов выборки, принимающая значения в $\Theta$*. **Принято обозначать как $\overline{\theta}_n$ (вместо линии должна быть крышека сверху ^)**.
Но как нам понять, что оценка выбрана хороша и вообще имеет смысл? Например мы могли бы взять для оценки среднего выборки $x_1 * x_n$, оно попадает под определение, но очевидно на среднее выборки совсем не тянет.

**Оценка** должна быть не просто **функцией** от элементов выборки, а **от случайных величин, которые эти числа породили**. 
Это не просто число (значение), от конкретных элементов выборки, которые мы пронаблюдали, а **функция от всех возможных наборов**, которые могут получиться от реализации случайных величин $\xi_1(w) = x1 ... \xi_n(w) = x_n$.
**Тогда нас интересует случайная величина** $\overline{\theta}_n (\xi_1...\xi_n)$.

**Характеристики качества оценки**:
1) Несмещённая оценка
   $E\overline{\theta}_n = \theta$
   Оценка несмещённая, если в среднем она даёт в точности неизвестный параметр $\theta$
2) Состоятельная оценка
   Оценка состоятельна, если случайная величина сходится по вероятность при n стремящемся к бесконечности на всё параметрическом множестве.
   $\overline{\theta}_n \to^P \theta$  при $n \to \infty$, где $\theta \in \Theta$
   Сходимость по P (по вероятности) это: $\forall \varepsilon > 0 \space P(|\overline{\theta}_n - \theta| > \varepsilon) \to 0, при \space n \to \infty$

**Примером оценки**, которая является не несмещённой но выглядящей логично является **дисперсия**. 
Допустим мы хотим оценить дисперсию случайной величины:
$\xi_i \sim N(a, \theta^2)$ 
Возьмём оценку параметра по формуле дисперсии:
$\overline{\theta}_n = \frac{1}{n} \Sigma_i^n(\theta_i - a)^2$, если $a$ неизвестен, то $\xi_i \sim N(\theta_i, \theta^2)$ и получим:
$\overline{\theta}_n = \frac{1}{n} \Sigma_i^n(\theta_i - \frac{1}{n_j} \Sigma_j^n\theta_j)^2$ 
Вычислим мат ожидание: 
$E\overline{\theta}_n = \frac{n-1}{n} \theta_2^2$ 
Получается, что оценка не несмещённая. А чтобы получить несмещённую оценку, вместо формулы дисперсии нужно взять её с небольшим изменением:
$\overline{\theta}_n = \frac{1}{n-1} \Sigma_i^n(\theta_i - \frac{1}{n_j} \Sigma_j^n\theta_j)^2$. 
И тогда получим несмещёную оценку.
Это случилось потому, что среди слагаемых в формуле (1) есть линейная зависимость (до конца не понял это).
### Функция риска
Как нам измерить насколько мы ошиблись, сделав оценку нашего параметра?
Для этого вводится **функция штрафа** (потерь):
$u(.)$
Со свойствами:
1) u(0) = 0
2) u(x) = u(-x)
3) монотонная
Примеры функций: $u(x) = x^2 , \space u(x) = |x|$
И для вычисления ошибки нашей оценки параметра будем подставлять в функцию штрафа оценку и сам параметр. Тогда **функция штрафа** примет вид:
$$u(\theta_n - \theta)$$

Но нам нужно как-то интегрально (не поточечно, а на всём отрезке) посмотреть на ситуацию, то есть оценить функцию штрафа не точечно, а на каком-то диапазоне. Для этого вводим **функцию риска**:
$$ R_{\overline{\theta}_n} (\theta) = Eu(\overline{\theta}_n - \theta)$$
то есть это математическое ожидание штрафа.
А если оценка параметра *несмещённая*, то: 
$$ R_{\overline{\theta}_n} (\theta) = Eu(\overline{\theta}_n - E\overline{\theta}_n)$$
А если ещё функция штрафа квадратичная, т.е. $u(x) = x^2$, то функция риска является дисперсией оценки параметра:
$$ R_{\overline{\theta}_n} (\theta) = D\overline{\theta}_n$$

Подходы к оценке функции риска:
1) Интегральный (байесовский подход)
   $\int_\Theta R_{\overline{\theta}_n}(\theta)d(\theta) \to min_{\overline{\theta}_n}$
2) Минимаксный подход
   $max_{\theta \in \Theta}R_{\overline{\theta}_n}(\theta) \to min_{\overline{\theta}_n}$
**Пример**:
Допустим, что случайная величина принадлежит нормальному распределению с такими параметрами: $\xi_i \sim N(\theta, \theta^2)$
И мы хотим оценить параметр $\theta$ (т.е. мат ожидание), тогда мы можем взять оценку параметра из класса функций: $$\overline{\theta}_n \in \{ \mu_n \Sigma_i^n \xi_i\}$$, где  $\mu_n$ - это просто число зависящее от n.

 То есть мы не сразу берём привычную нам формулу для мат ожидания $\overline{\theta}_n = \frac{1}{n} \Sigma_i^n \xi_i$, а мы сначала предполагаем, что оценка параметра принадлежит выбранному классу функций, а потом подбираем параметр (в данном случае $\mu_n$), чтобы функция риска от этого параметра имела лучшее значение. А это лучшее значение выбирается с помощью подходов описанных выше.

Мы предполагали, что оптимальная оценка для мат ожидания будет с $\mu_n = \frac{1}{n}$, но после вычисления функции риска и применения подходов для нахождения параметра оценки $\mu_n$ выяснилось, что на самом деле оптимальная оценка с $\mu_n = \frac{1}{n+1}$.
## Эмпирическая функция распределения
Вот у нас есть вся функция распределения. Как угадать, оценить какое у нас распределение?
Для этого используется **эмпирическая функция распределения**. То по элементам выборки мы хотим воссоздать функцию распределения. То есть мы получаем оценку функции распределения:
$\overline{F}_n(x, \xi_1, ..., \xi_n) = \frac{1}{n} \Sigma_{i=1}^n I_{\{ \xi_i \leq x\}}$
это пример для дискретного случая.
А в целом у функции распределения формула:
$F_\xi(x) = P(\xi \leq x)$

#замечание Слово "статистика" часто используется со значением "оценка", то есть какой-то объект, любая функция от элементов выборки.

**Важные свойства**:
1) Для каждого x наша **оценка** функции распределения является **несмещённой**:
   $\forall x \in R \space \overline{F}_n(x, \xi_1, ..., \xi_n)$ - несмещённая оценка истинной функции распределения $F_{\xi_i}(x)$ (из которой Бог получается случайные значения)
2) **Оценка** функции распределения является **состоятельной**.
   $\forall x \in R \space \overline{F}_n(x, \xi_1, ..., \xi_n)$ - состоятельная оценка для $F_{\xi_i}(x)$
Проблема пунктов выше, что мы смотрим только на точечные оценки функции распределения. Поэтому сейчас сформулируем теорему, что эта **оценка хорошо апроксимирует функцию распределения на всей прямой сразу**:

**Теорема**. Гливенко-Кантелли
Если мы возьмём вероятность того, что sup (супремума) по всем x из R модуля разности оценки функции распределения и истинной функции распределения при n -> бесконечности будет стремиться к нулю, то это вероятность будет равна 1.
$$ P(sup_{x \in R} |\overline{F}_n(x, \xi_1,...,\xi_n) - F_{\xi_i}(x)| \to_{n \to \infty} 0) = 1 $$
То есть для почти любой выборки, которая можно породиться в рамках эксперимента, не только разность между оценкой и распределением близка к нулю, а даже супремум по все прямой стремиться к нулю. 
То есть эта **теорема** уже **даёт необходимую апроксимацию** оценки распределения и самого распределения **на всей прямой**, о которой говорилось выше. То есть практически нет шанса того, что наша эмпирическая функции хоть где-то отклониться от истинной функции распределения.

**Связь центральной предельной теоремы и закона больших чисел**.
**ЗБЧ**:
$\frac{\xi_1 + ... + \xi_n - na}{n} \to 0$
**ЦПТ**:
$\frac{\xi_1 + ... + \xi_n - na} {\sqrt{n}\sigma} \to^d N(0, 1)$ - **сходится по распределению** к нормальному распределению. Это значит что при больших n функ распределения допредельной величины очень похожа на функ распределения справа.
Сходимость по распределению: 
$\xi_n \to^d \eta \Leftrightarrow P(\xi_n \leq y) \to_{n \to \infty} P(\eta \leq y)$. 
Эти определения выше даны, чтобы сформулировать теорему, которая соотносится с теоремой Гливенко-Кантелли так же, как ЦПТ соотносится с ЗБЧ:

**Теор**. Колмогорова
Пусть $F_{\xi_i}(x)$ - непрерывно и пусть:
$\sqrt{n}D_n = \sqrt{n} \space sup_{x \in R}|\overline{F}_n(x, \xi_1, ..., \xi_n) - F_{\xi_i}(x)|$, 
тогда:
$\sqrt{n}D_n \to_{n \to \infty}^d \eta$, 
где $\eta$ - это случайная величина с функцией распределения:
$F_\eta(y) =$ 
1) $\space 0 \space при \space y \leq 0, \space$ 
2) $\space \Sigma_{i=-\infty}^\infty(-1)^i e^{-2i^2y^2} \space при \space y > 0$

То есть 
$P(\sqrt{n}D_n \leq y) \to_{n \to \infty} F_\eta(y)$ (к функции распределения выше)

## Асимптотическая нормальность
Снова запишем, что **имеем**:
$\xi_1...\xi_n$
Их реализации:  
$x_1...x_n$
Функция распределения:
$F_{\xi_i} \in \{ F_\theta \}, \theta \in \Theta$

**Асимптотическая нормальность**.
$\overline{\theta}_n$  - асимптотически нормальная оценка, если:
$\sqrt{n} (\overline{\theta}_n - \theta) \to_{n \to \infty} \eta \sim N(0, \sigma^2(\theta))$
где $\sigma^2(\theta)$ - асимпт. дисперсия

Пусть 
- p(x) - плотность
- p(0) != 0
- p(x) - чётная функция
Например стандартная нормальная плотность или плотность распределения коши.

**Функция распределение**
$F(x) = \int_{-\infty}^xp(t)dt$
будет **симметричной**, если плотность такая, как описано выше.

Теперь пусть функция, которая породила выборку $\xi_i$:
$F_{\xi_i}(x) = F(x - \theta)$
, тогда $\theta$ - **это матожидание**.

Для всех таких распределений параметр $\theta$ так же является **медианой**.
$\theta = E\xi_i;$ 
$\theta = x_{1/2}$
медиана можно сказать это такая точка, что вероятность попасть слева или справа от неё на функции распределения одинаковы. То есть $F(\xi \leq x) = 1/2$, x - медиана и обозначается как $x_{1/2}$.

**Теор**. **О медиане**.
$\sqrt{n}(MED - x_{1/2}) \to_{n \to \infty} \eta \sim N(0, \frac{1}{4p^2(0)})$
$MED$ - медиана
Теорема утверждает, что когда мы находимся в ситуации выше, т.е. имеется чётная плотность, по ней построена функция распределения, а каждый элемент выборки имеет функцию распределения с неизвестным параметром сдвига $\theta$. 
То в этой конкретной ситуации, если мы возьмём наш параметр $\theta$ (и он как раз окажется равным теоретической медиане), сравним с медианой выборочной, которую можно определить так:
$\theta_n^2 = MED =$ 
1) $x_{(k)}, \space if \space n=2k-1$ 
2) $\frac{x_{(k)} + x_{(k+1)}}{2}, \space if \space n=2k$. 
То есть среди упорядоченных элементов выборки берём средний элемент для случаев нечётного и чётного количества элементов выборки.
То после домножения на нормирующий множитель $\sqrt{n}$, мы  получим в пределе нормальную случайную величину с такой вот плотностью.
### Как выбирать между двумя асиптотическими оценками
Ответ: лучше выбирать ту оценку у которой меньше асимптотическая дисперсия. Это потому, что чем меньше асипт дисперсия, тем оценка эффективнее. Эффективнее значит, чем эффективнее, тем меньшей объём выборки требуется, чтобы апроксимировать с помощью этой оценки неизвестный параметр с наперёд заданной точностью.

## Методы построения статистических оценок
### Метод моментов
**Предисловие**. У нас есть числовая выборка, которая порождена случайными величинами, которые мы верим взаимно независимы и одинаково распределены. А так же нам дано распределение этих случайных величин с точностью до m параметров $\theta_i$, которые живут в параметрическом множестве $\subset R^m$. Это нам дано изначально.
И теперь мы хотим придумать какие-нибудь хорошие оценку не на основе догадок, а на основе общих технологий.
**Интуиция за этой технологией**. Мы умеем оценивать моменты ($E\xi_i^k$ ) с помощью хороших состоятельных несмещённых оценок из элементов выборки ($\overline{x^k}$). 
Потом мы составляем систему уравнений ниже, решаем её, предполагая, что отображения, которые система задаёт взаимно однозначны и взаимно непрерывны, поэтому решение существует и единственно. Говорим, что это решение является оценкой параметров, полученное с помощью метода моментов. И в общем случае, когда параметров несколько, мы гарантируем состоятельность этих оценок, которые мы получили как решение системы.
**В двух словах**. Если мы уже знаем какие-то состоятельные оценки, то решая подобные системы можно получать состоятельные оценки для других параметров.

**Сам метод**:
Вот мы знаем или угадали некую оценку для матожидания и мы убедились, что она хорошая:
для $E\xi_i$  $\overline{x}=1/n*\Sigma_{i=1}^2\xi_i$
для $E\xi_i^2$  $\overline{x^2}=1/n*\Sigma_{i=1}^2\xi_i^2$
...
для $E\xi_i^k$ $\overline{x^k}=1/n*\Sigma_{i=1}^2\xi_i^k$
И эти оценки выше проверены и рабочие.

У нас как обычно есть:
$x_1...x_n$
$\xi_1...\xi_n$
$F_{\xi_i} \in \{ F_\theta \}, \theta \in \Theta \subset R$

И если я посчитаю матожидание $E\xi_i^k$, я получу какую-то функцию от параметров
$E\xi_i^k = f_k(\theta_1,...,\theta_k)$

Теперь составим систему уравнений из этих функций относительно неизвестных параметров распределения:
$f_1(\theta_1,...,\theta_m) = \overline{x^1}$
$f_2(\theta_1,...,\theta_m) = \overline{x^2}$
...
$f_m(\theta_1,...,\theta_m) = \overline{x^m}$
И предположим, что многомерное отображение заданное функциями f_1,...,f_m **взаимно однозначно** и **взаимно непрерывно**, тогда **решение единственно и существует**.
И это **решение** мы назовём **оценкой** полученной **методом моментов**:
$\overline{\theta}_n^1, \overline{\theta}_n^2, ..., \overline{\theta}_n^m$

**Например**:
Пусть m=1, то есть у нас только один параметр, тогда $f_1(\theta) = \overline{x}$, а $f_1(\theta)$ - это мат ожидание нашей случайной величины по определению выше. И мы знаем, что $\overline{x}$ - это состоятельная оценка для мат ожидания, то есть по вероятности $\overline{x}$ сходится к математическому ожиданию, а это то же самое, что для такого взаимно однозначного и непрерывного отображения:
$f_1^{-1}(\overline{x}) \to^P \theta$
То есть **получили состоятельную оценку** неизвестного нам **параметра**.
**В этом и есть идея метода**. Решая систему выше мы получаем состоятельные оценки для неизвестных параметров распределения.
### Метод максимального правдоподобия
**Предисловие**.
Он в каком-то смысле более мощный. Здесь, в отличие от метода моментов, никакой догадки, априорных знаний нам не нужно.

**К методу**.
По жизни (с которыми обычно работаем) бывают два типа распределений: дискретные и абсолютно непрерывные. Поэтому **введём в функцию**, которая зависит от типа распределения, с которым работаем:
$f(x;\theta):=$ 
1) $P_\theta(\xi=x) \space if \space дискретное$ 
2) $p_\xi(x; \theta), \space абс \space непрерыв$
**Функция правдоподобия**:
$L(x_1, ..., x_n, \theta)= \Pi_{i=1}^nf(x_i,\theta)$
**Смысл такой**, что мы либо перемножаем в дискретном случае вероятности, с которыми $\xi_i = x_i$, либо перемножаем плотности. То есть это просто либо вероятность того, что $\xi_1=x_1$ и так далее $\xi_n=x_n$, либо совместная плотность $p_{\xi_1}(x_1, \theta)$ и так далее в точке нашей выборки $x_1$ и так далее.
**В двух словах**. Можно сказать, что это совместная вероятность. Чем больше она (вероятность), что именно такая случ величина $\xi_i$ принимает такое значение $x_i$ и так далее, при выбранных параметрах, тем правдоподобнее эти параметры, то есть тем вероятнее, что именно с этим параметром и задана функция распределения, которая есть у господа Бога.

**Оценка максимального правдоподобия** (ОМП).
Это точка $\overline{\theta}_n$ максимума функции L.

Для нахождения максимума удобнее всего через экстремумы функции, то есть через производную при приравнивании к нулю. Но дифференцировать L тяжело, поэтому лучше взять логарифм от L. Точка максимума будет одна и та же, так как логарифм монотонная функция.
Получаем систему:
$\frac{\partial LnL}{\partial\theta_L} = 0$
$\frac{\partial LnL}{\partial\theta_m} = 0$

**Например** функция правдоподобия для биноминального распределения $Binom(k, \theta)$:
Функция $f(x, \theta)$ будет $P(\xi_i = x_i) = C_k^{x_i}\theta^{x_i}(1-\theta)^{k-x_i}$
И функция правдоподобия:
$L(x_1, ..., x_n, \theta)= \Pi_{i=1}^nC_k^{x_i}\theta^{x_i}(1-\theta)^{k-x_i}$
Обернув это в логарифм и решив систему диф уравнений как выше получим следующие оценки параметров:
$\overline{\theta}_n = \frac{1}{kn}\Sigma_{i=1}^nx_i$
## Условия регулярности
### Условия
1) Первое условие
   Введём такое множество:
   $A := \{ x: \space f(x,\theta) \neq 0\}$
   важно, $A$ не зависит от $\theta$ 
2) $f(x,\theta)$ дифференцируема по $\theta$ на $A$.
3) Вводим функцию: $U=\partial \frac{lnf(x,\theta)}{\partial\theta} = U(\xi, \theta)$
   А теперь условие: $EU=0 \space \forall \theta \in \Theta$, $0 < DU < \infty$ ($D$ - дисперсия)

Объяснение 3-го условия. Мат ожидание.
И замечание: интеграл ниже заменяется на сумму, если у нас дискретный случай.
$EU=\int_A \frac{\partial lnf(x; \theta)}{\partial \theta} f(x; \theta) dx = \int _ A \frac{\partial f}{\partial \theta} \space dx = 0$
т.к. $f(x;\theta)$ представляет плотность, то
$\frac{\partial}{\partial \theta} (\int_A f(x; \theta) \space dx = 1) = 0$.
То есть в каком-то смысле **3-ье условие это перестановочность производной и интеграла**.

Объяснение 3-го условия. Дисперсия.
$DU = E (\frac{\partial lnf(\xi; \theta)}{\partial \theta})^2$
По опред $D\xi = E(\xi^2) - (E\xi)^2$, но так в условии выше $EU = 0$ в дисперсии остаётся только 1-ое слагаемое.
### Информация фишера
Условие выше для дисперсии назовём информацией фишера. И обозначим как $I_1(\theta)$:
$DU = E (\frac{\partial lnf(\xi; \theta)}{\partial \theta})^2 = I_1(\theta)$

Это некая функция информации от элементов выборки. Её так же обозначают как $I_n(\theta)$, что является:
$I_n(\theta) = nI_1(\theta)$
Можно сказать информация по всей выборке это:
**Определение информации фишера** для n элементов.
$I_n(\theta) = E(\frac{\partial lnL(\xi_1, ..., \xi_n, \theta)}{\partial \theta})^2$

Это определение отличается от записи дисперсии выше только тем, что там f от одного $\xi$, а $L$ - это произведение таких f, но когда мы логарифимируем это $L$ мы получаем сумму логарифмов (а потом сумма производных от этих логарифмов), но **дисперсия суммы это сумма дисперсий**, т.к. **случайные величины независимы**.
То есть $I_n$ это есть дисперсия, потому что из него можно ещё вычитать $(E\xi)^2$, но мат ожидание ноль из 3-го условия

### Неравенство Рао-Крамера
**Теор**.
Пусть выполняются условия 1-3 (регулярности).
Пусть $\overline{\theta}_n$ некоторая несмещённая оценка неизвестного параметра $\theta$. Пусть мат ожидание этой оценки можно дифференцировать по $\theta$ под знаком интеграла.
$E\overline{\theta}_n = \int_R \overline{\theta}_n(x_1,...,x_n) L(x1,...,x_n, \theta) dx_1,...,dx_n$ вот его.
**Тогда** дисперсия оценки параметра (а это содержательная характеристика как мы выяснили выше, это в каком-то смысле мера эффективности этой оценки):
$D\overline{\theta}_n \geq \frac{1}{I_n(\theta)}$

**Смысл теоремы выше**.
То есть эта теорема говорит, что если модель регулярная, то есть у неё и недостатки. А именно, что мы не сможем придумать оценку со слишком быстро стремящейся к нулю дисперсией, то есть у оценки есть ограничение по эффективности.

### Теорема об оценке макс правдоподобия в уловиях регулярности
**Теор**.
Пусть выполнены услов 1-3 (регулярности).
Пусть $f(x;\theta)$ трижды дифференцируема по $\theta$, причём $\frac{\partial^3 f(\xi; \theta)}{\partial \theta^3} \leq h(\xi) \space : \space Eh < \infty$
**Тогда** решение уравнения:
$\frac{\partial lnL}{ \partial \theta} = 0$
является оценкой максимального правдоподобия (ОМП).

## Интервальные оценки. Доверительный интервал
**Вступление**.
До этого все темы с оценками понимались как точечные оценки, то есть оценки, которые своим значением точечно должны были приблизиться к оцениваемому параметру, а уже потом мы должны были оценить качество этой оценки с помощью несмещённости, состоятельности и так далее.
А **интервальные оценки** говорят о том, что мы не одной функций хотим приблизиться к параметру, а двумя, т.е. зажать его между двумя функциями, чтобы с высокой вероятностью он там лежал. Это называется **доверительный интервал**.

Опр. **Доверительный интервал**.
Пусть есть уровень доверия  $\alpha \in (0, 1)$ 
Доверительным интервалом с доверительной вероятностью $\alpha$ называется пара случайных величин $\overline{\theta}_n^1(\xi_1,...,\xi_n), \overline{\theta}_n^2(\xi_1,...,\xi_n)$ такая что:
$$P(\theta \in [\overline{\theta}_n^1(\xi_1,...,\xi_n),\overline{\theta}_n^2(\xi_1,...,\xi_n)]) \geq \alpha$$
В теории задача стоит, чтобы построить как можно меньший интервал.

**Асиптотический доверительный интервал**.
$$P(\theta \in [\overline{\theta}_n^1(\xi_1,...,\xi_n),\overline{\theta}_n^2(\xi_1,...,\xi_n)]) \sim_{n \to \infty} \alpha$$
**Пример** применения:
Пусть есть случайная величина $\xi_i \sim N(\theta,\sigma^2)$
Так как нам известны квантили стандартного нормального распределения, поэтому
приведём случ величину к его виду: $\frac{1}{\sqrt{n}} \Sigma_{i=1}^n\frac{\xi_i - \theta}{\sigma} \sim N(0,1)$
  
Нам дано $\alpha$ , та доверительная вероятность, которую мы хотим получить, мы берём любые числа так, чтобы:
$\gamma, \beta : \gamma - \beta = \alpha$
И так квантили стандартного нормального распределения мы знаем (из табличек или зашито в программу), мы можем их получить для $\gamma, \beta$. 
Квантили это точки $u_{\gamma}, u_{\beta}$, такие что $P(u_{\gamma}) = \gamma, \space P(u_{\beta}) = \beta$.

А раз эти точки квантили, то можем получить вероятность
$$P(u_{\beta} \leq \frac{1}{\sqrt{n}} \Sigma_{i=1}^n(\frac{\xi_i - \theta}{\sigma} \sim N(0,1) \leq u_{\gamma}) = \gamma - \beta = \alpha$$
Из этого выражения можно получить ограничения сверху и снизу для параметра $\theta$, это и будет **доверительный интервал**.

## Проверка статистических гипотез
### Гипотезы о согласии
Это гипотеза о том, что наша выборка согласуется с определённой функцией распределения.
Как обычно **имеем**:
$x_1...x_n$
$\xi_1...\xi_n$
$F_{\xi_i} \in \{ F_\theta \}, \theta \in \Theta$

И есть **гипотеза**:
$H_0$
она основная и предполагает, что $F_{\xi_i} = F$
А альтернативной гипотеза $H_1$ $F_{\xi_i} \neq F$

Возможные ошибки:
Ошибка $I$ рода: отвергаем $H_0$, хотя она верна
Ошибка $II$ рода: принимаем $H_0$, хотя верна $H_1$

Давай выберем множество точек:
$\Omega \subset R^n$
и назовём **критическим множеством**.
И критерий опишем так:
Если $(x_1,...,x_n) \in \Omega$, то гипотезу **отвергаем**.
Если $(x_1,...,x_n) \notin \Omega$, то гипотезу **принимаем**.
А как выбрать эту $\Omega$ это вопрос.

Если $\Omega$ выбрана, то точно можно посчитать **формулу вероятность ошибки $I$ рода** (то есть гипотеза верна, но вектор всё равно попал в критическое множество). А если $H_0$ верна, то вероятностная мера $P_0$ порождается функцией распределения $F$ (т.к. нулевая гипотеза верна). И получаем формулу:
$P_0((\xi_1,...,\xi_n) \in \Omega)=\int_{\Omega}p_{\xi_1,...\xi_n}(x_1,...,x_n)$

#### Критерий колмогорова 
**Теорема колмогорова (повторение)**. 
$F$ - непрерывная
$\sqrt{n}D_n = \sqrt{n}*sup_{x \in R}|\overline{F}_n(x, \xi_1, ..., \xi_n) - F_{\xi_i}(x)|$,
тогда:
$\sqrt{n}D_n(\xi_i,...,\xi_n) \to_{n \to \infty}^d \to \eta$, 
где $\eta$ - это случайная величина с функ распределения (колмогоровским):
$F_\eta(y) =$
1) $0 \space при \space y \leq 0$
2) $\Sigma_{i=-\infty}^\infty(-1)^i e^{-2i^2y^2} \space при \space y > 0$

Так как у случайной величины колмогоровское распределение, квантили этой функции распределения известны. Можно обозначить так. То есть если мы хотим померять вероятность для случайной величины $\sqrt{n}D_n(\xi_i,...,\xi_n)$, то её можно мерять в терминах $K(y)$, где:
$F_\eta(y) = K(y)=$
1) $0 \space при \space y \leq 0$
2) $\Sigma_{i=-\infty}^\infty(-1)^i e^{-2i^2y^2} \space при \space y > 0$

**Дальше**:
$\overline{F}_n(x, \xi_1, ..., \xi_n)$ - наша выборка
$H_0: \space F_{\xi_i}(x)=F(x)$, 
то есть предположили, что у нашей выборки распределение $F_{\xi_i}(x)$
Посчитали
$D_n=sup_{x \in R}|\overline{F}_n(x, \xi_1, ..., \xi_n) - F_{\xi_i}(x)|$

По логике кажется, что $D_n$ маленькое, то гипотеза правильная. А если он большой, то стоит отвергнуть гипотезу.

**Тогда**:
В качестве $\Omega$ стоит взять такие точки:
$\Omega_C = \{(x_1,...,x_n): \space  \sqrt{n}D_n(x_1,...,x_n) > c \}$
Получается от $c$ зависит порог принятия решения (когда отвергаем гипотезу). И в зависимости от того **как мы выберем эту константу будет зависеть вероятность ошибки** (первого рода). Распишем это:
$P_0((\xi_1,...,\xi_n) \in \Omega) \sim P(\eta > c)$, что равно $= 1 - K(c)$

Эта точка
$c_{1-\alpha}: \space K(c_{1-\alpha}) = 1 - \alpha$ 
Будет $1 - \alpha$ квантиль колмогоровского распределения.
И если мы так выбираем c, то $\Omega$ переименуем в:
$\Omega_{\alpha} = \{(x_1,...,x_n): \space  \sqrt{n}D_n(x_1,...,x_n) > c_{1-\alpha} \}$
И эта точка $c_{1-\alpha}$ - критическая точка для $\alpha$ или же квантиль для $1 - \alpha$.

**Вывод**.
Гипотеза тем разумнее, чем выше для неё порог $\alpha$

#### Критерий $\chi^2$
У нас опять выборка:
$x_1,...,x_n$
и гипотеза:
$H_0: \space F_{\xi_i}=F$

Для примера предположим, что $F = Bin(k, p)$
**Обозначим** множество:
$\Delta = \{ 0,1,2,3 ...,k\}$ 
это множество всех точек, которые случайная величина принимает с ненулевой вероятностью.

Возьмём $N$ кусочков этого $\Delta$:
$\Delta = \Delta_1 \cup ... \cup \Delta_N$
очевидно, что нет такого элемента выборки, которые бы не попало в одно из $\Delta_i$, иначе наша гипотеза идиотская (неправильно предположили распределение $F$)

Обозначим:
$\nu_1$ - кол-во элементов выборки в $\Delta_1$ 
и т.д.
$\nu_1...\nu_N=n$
Причём эти $\nu_i$ - случайные величины, т.к. зависят от $x$.

Введём вероятность $p_i$ попадания какой-то случ величины $\xi_i$ в определённую $\Delta_i$, если $H_0$ верна. Самый простой вариант этого: 
$p_i = \nu_i / n$ 

**Теорема Пирсона**.
$$\Sigma_{i=1}^N \frac{(\nu_i - np_i)^2}{np_i} \to_{n \to \infty}^d \eta \sim \chi_{N-1}^2$$

на этот раз $\eta$ - это распределение $\chi_{N-1}^2$ (хи-квадрат распределение с N-1 степенями свободы).

**Вывод** по теореме пирсона.
Это снова статистика, которая меряет разницу между эмпирическими данными и гипотезой.
И чем больше эта величина, тем гипотеза глупее, и чем меньше - тем разумнее.

При достаточно больших n (объёме выборки) можно пользоваться квантилями $\chi^2$ распределения.
И логика такая же как у критерия колмогорова. Строим критическую область, где $\alpha$ уровень значимости критерия:
$\Omega_{\alpha} = \{(x_1,...,x_n): \space  \Sigma_{i=1}^N \frac{(\nu_i - np_i)^2}{np_i} > \chi_{N-1, 1-\alpha}^2 \}$
И вся логика тут как и у критерия колмогорова.

### Гипотезы с альтернативой. Критерий неймана-пирсона
**Предисловие**.
Раньше у нас была только нулевая гипотеза, а если она отвергалась, то чёрт его знает какая альтернатива.
Теперь у нас будет и альтернатива. Будут две конкурирующие между собой гипотезы.

Как обычно есть:
$x_1,...,x_n$
$\xi_1...\xi_n$
$F_{\xi_i} \in \{ F_\theta \}, \theta \in \Theta$

Будем считать, что $\theta = \{\theta_0, \theta_1 \}$
Где основная гипотеза:
$H_0: \space \theta = \theta_0$
а альтернативная:
$H_1: \space \theta = \theta_1$

**Пример**.
$\xi_i \sim N(\theta, 1) \space H_0: \theta_0=0.3, H_1: \theta_1=0.7; \alpha \in (0,1)$
$D_{\alpha}:= \{(x_1,...,x_n): \space \frac{L(x_1,...,x_n,\theta_1)}{L(x_1,...,x_n,\theta_0)} \geq c_{\alpha}\}$
где $L$ - функция правдоподобия, $D_\alpha$ - критическая область.

Так как функция правдоподобия можно сказать показывает вероятность, что именно при данном параметре появляются такие наблюдаемые значения выборки из рассматриваемой функции распределения. И параметр звучит как если отношение функ правдоподобия с параметром $\theta_1$ к функ правдоподобия с параметром $\theta_0$ достаточно велико (больше $c_\alpha$), то лучше предпочесть параметр $\theta_1$. 

После некоторых преобразований отбор в критическую область примет вид:
$D'_\alpha = \{(x_1,...,x_n): \Sigma_{i=1}^n x_i \geq b_\alpha \}$

#замечание. Лучше допустить ошибку 1-го рода, чем ошибку 2-го рода.
Поэтому мы хотим заранее задаться вероятностью ошибки $I$-го рода и исходя из этого найти $b_\alpha$. А ошибку 2-го рода сделать как можно меньше.
**P(ошибки 1-го рода)** = $P_0((\xi_1,...,\xi_n): \space \Sigma_{i=1}^n \xi_i \geq b_\alpha)$,   $\xi_i \sim N(30, 100)$
$=P_0((N(30, 100) - 30)/10 > (b_\alpha - 30)/10) = \alpha$

то есть это вероятность того, что случайные величины с распределением с параметром $\theta_0$ попадут в критическую область (что соответствует $\theta_1$). И мы привели случайную величину от $N(30,100)$ к $N(0.3, 1)$

А вероятность ошибки 2-го рода примет вид:
$P_1(\Sigma_{i=1}^n \xi_i < b_\alpha) = P_1((N(70, 100) - 70) / 10 < (b_\alpha - 70) / 10)$

А теперь на основании вышеописанного
**Теорема**.
Пусть для данного $\alpha \in (0,1)$, $p_\alpha$ - это вероятность ошибки 2-го рода, найденная в рамках нашего примера.
Пусть $C_\alpha$ - любое множество в $R^n$, такое что:
$P_0((\xi_1,...,\xi_n) \in C_\alpha) = \alpha$
это вероятность ошибки 1-го рода.
Тогда:
$P_1((\xi_1,...,\xi_n) \notin C_\alpha) \geq C_\alpha$

То есть эта теорема говорит о том, что **критерий неймана-пирсона гарантирует минимальность ошибки 2-го рода** при заданной вероятности ошибки 1-го рода. 
То есть найденная область $D_\alpha$ - эта та область, на которой минимизируется ошибка 2-го рода

#### Рандомизированный критерий
Пусть есть 3 ситуации:
- $\Sigma_{i=1}^n \xi_i \geq a_\alpha + 1$ => отвергаем $H_0$
- $\Sigma_{i=1}^n \xi_i \leq a_\alpha - 1$ => отвергаем $H_1$
- $\Sigma_{i=1}^n \xi_i = a_\alpha$ с помощью монетки решаем (решка $p_\alpha^*$=> принимаем $H_0$, орёл $1-p_\alpha^*$ => отвергаем $H_0$) 

### Гипотезы однородности
Пусть у нас есть k выборок:
$x_1^1,...,x_{n_1}^1;x_1^2,...,x_{n_2}^2;x_1^k,...,x_{n_k}^k$
Размеры выборок:
$n=n_1+...+n_k$
Элементы выборки порождены случайными величинами:
$\xi_1^1,...,\xi_{n_1}^1;\xi_1^2,...,\xi_{n_2}^2;\xi_1^k,...,\xi_{n_k}^k$

**Гипотеза** $H_0$ - у **выборок одинаковые распределения** (т.е. согласованы между собой).
Но только для того, чтобы хотя бы высказать гипотезу, данные не должны быть сильно зашумлены.

#### Критерий Колмогорова-Смирнова
Для **примера** пусть k = 2:
$\overline{F}_n$ - э.ф.р. для 1-ой выборки
$\overline{G}_n$ - э.ф.р. для 2-ой выборки

Как и в случае для обычного критерия Колмогорова мы предполагаем, что неизвестные нам распределения для  $\xi_i$ и $\eta_i$ непрерывные.
Без предположения непрерывности критерий колмогорова не работает.
**Утверждение теоремы**.
Если у нас случайные величины  $\xi_i$ и $\eta_i$ одинаково и непрерывно распределены (то есть справедлива $H_0$), тогда:
$$D_{n,m}(x_1,...,x_n,y_1,...,y_n,x) = \sqrt{\frac{n*m}{n+m}} sup_{x \in R}|\overline{F}_n(\xi_1,...,\xi_n, x) - \overline{G}_n(\eta_1,...,\eta_m,x)| \to_{n,m \to \infty}^d \zeta \sim K$$
$K$ - колмогоровское распеделение.

И если на наших выборках эта статистика оказалась больше:
$D_{n,m}(x_1,...,x_n,y_1,...,y_n,x) > \kappa_{1-\alpha}$
где $\kappa_{1-\alpha}$ это $1-\alpha$ квантиль колмогоровского распределения. 
Так вот если значение этой статистики больше, чем этот квантиль, то на уровне $\alpha$ мы отвергаем основную гипоезу.

Тогда вероятность ошибки 1-го рода (приняли $H_1$ , но верна $H_0$):
$P_0(D_{n,m}(x_1,...,x_n,y_1,...,y_n,x)) = \alpha$ (примерно равно)

#замечание В критериях согласия **колмогоров** для непрерывных ситуаций, а $\chi^2$ для **дискретных**.

