# Алгоритм Graph Convolutional Network (GCN) и лаплассиан графа
### Что такое лаплассиан в контексте графов?

Пусть у нас есть граф $G=(V,E)$, где $V$ — множество вершин, а $E$ — множество рёбер. Матрица смежности этого графа обозначается через $A$, где элемент $a_{ij}$​ равен 1, если существует ребро между вершинами $i$ и $j$, и 0 в противном случае. Также введём диагональную матрицу степеней $D$, где каждый элемент $d_{ii}$​ равен степени вершины $v_i$​.

Тогда нормированный лапласиан графа $L$ определяется как:
$$L=I−D^{−\frac{1}{2}}AD^{−\frac{1}{2}}$$
где $I$ — единичная матрица.

### Применение лаплассиана в Graph Convolutional Network (GCN)

Алгоритм GCN использует лапласиан для выполнения свёртки над графовыми структурами. Основная идея заключается в том, чтобы обобщить концепцию свёрточной нейронной сети (CNN) на нерегулярные структуры данных, такие как графы.

Для этого вводится операция свёртки на графе, которая учитывает структуру связей между вершинами. Формула свёрточного слоя в GCN выглядит так:
$$H^{(l+1)}=σ(\overline{D}^{−\frac{1}{2}}\overline{A} \space \overline{D}^{−\frac{1}{2}}H^{(l)}W^{(l)})$$
Здесь $\overline{A}=A+I$ — дополненная матрица смежности (добавлены петли к каждому узлу), $\overline{D}$ — соответствующая диагональная матрица степеней, $H^{(l)}$ — матрица признаков узлов на слое $l$, $W^{(l)}$ — обучаемые веса, а $σ$ — нелинейность (например, ReLU).

Эта формула показывает, как происходит агрегация информации от соседей каждого узла и её преобразование при помощи весовой матрицы WW. Таким образом, использование лаплассиана позволяет эффективно учитывать локальную топологию графа при обработке данных.
### Зачем нужен лаплассиан?

Использование лаплассиана обеспечивает несколько важных свойств:
1. **Инвариантность к перестановкам**: Нормализованный лапласиан инвариантен относительно изменения нумерации вершин графа, что важно для обобщаемости модели.
2. **Спектральная теория графов**: Спектральные свойства лаплассиана позволяют анализировать графовые структуры с точки зрения их собственных значений и собственных векторов, что даёт важные сведения о структуре графа.
3. **Регуляризация**: Использование лаплассиана способствует сглаживанию функций на графах, что помогает избежать переобучения и улучшает обобщающую способность моделей.

# Пулинг графа (агрегация узлов)
Ссылка на описание пулинга графов: https://giga.chat/link/gcsVKLUWco