В продолжение темы [[Как представить граф в числовом виде]], возникает вопрос как обработать эту трёхмерную матрицу, которую мы получили в файле по ссылке и которая является временным рядом развития графа.

# Ответ ГПТ

### Трёхмерные матрицы и трансформеры
Трансформеры традиционно работают с двумерными матрицами, где одна ось представляет временные шаги, а другая — различные признаки или векторы (например, эмбеддинги). Однако, чтобы использовать třехмерную матрицу, можно рассмотреть несколько подходов:
1. **Непрямая интерпретация через агрегацию:**
    - **Агрегирование эмбеддингов:** Вместо подачи K эмбеддингов на каждой временной точке можно рассмотреть возможность агрегации этих эмбеддингов в один вектор. Например, можно воспользоваться методами, такими как среднее значение, максимум или же взвешенное среднее. Это позволит вам сократить размерность вашего входа до формы (t, n), где n — длина эмбеддинга, что будет совместимо с ожидаемым форматом входа трансформеров.
2. **Использование 3D-преобразования с добавлением временной оси:**
    - **Применение специальной проекции:** Если вы хотите сохранить информацию о нескольких эмбеддингах, можно использовать подход, похожий на тот, который применяется в обработке 3D-данных. Одним из подходов является добавление дополнительного измерения, и затем применение сверточных слоев для извлечения признаков из этой 3D-матрицы. Например, Conv3D может быть использован, чтобы обрабатывать вашу матрицу (t, K, n) и извлекать полезные паттерны для дальнейшей классификации или регрессии.
3. **Перемещение на уровне слоёв:**
    - **Multi-head attention:** Если вы хотите задействовать полную мощь трансформеров, можно рассмотреть возможность обработки K эмбеддингов как множества отдельных сэмплов в одном временном шаге, используя многоголовое внимание (multi-head attention). Каждое внимание может фокусироваться на разных аспектах представления узлов.
4. **Системы последовательной обработки:**
    - **Варианты RNN и LSTM:** Вы также можете использовать подход, где ваша 3D матрица будет развернута через рекуррентные нейронные сети, такие как LSTM или GRU, которые могут лучше обрабатывать последовательные данные из временных точек. В этом случае вы можете использовать метод внимания для получения информации из K узлов в каждом временном шаге.

### Комбинирование методов
Вариант освежить идеи заключается в том, чтобы комбинировать трансформеры с другими архитектурами. Например, вы можете использовать CNN на K эмбеддингах, чтобы извлечь признаковую информацию, а затем подать сжатые признаки в трансформер для обработки последовательности.

### Заключение
Вы приведли интересный способ обработки динамических графов с помощью эмбеддингов узлов и PageRank. Научные работы и практические исследования всё чаще исследуют использование графовых нейронных сетей и трансформеров для работы с графами и временными рядами, поэтому внимание к развитию новых архитектур может быть полезным. Также вы можете обратить внимание на существующие модели, которые были специально разработаны для работы с графами и временными рядами, такие как TGN (Temporal Graph Networks), которые объединяют принципы графов и временной зависимости.
Этот подход позволит вам эффективно использовать мощь трансформеров для обработки временных графов в трехмерной матрице, извлекая ценную информацию о динамике графов.

# Мои мысли
Можно попробовать к каждой точки временного ряда, то есть для каждой матрицы размера $(K, n)$ применять слой attention, а потом полносвязный слои/слои (возможно байесовские), сводящие результат attention к одному признаку.
Таким образом преобразование точки временного ряда можно представить так:
$(K, n)$ $\to_{attention}$ $(K, q)$ $\to_{FNN}$   $(K, 1)$
После преобразования каждой точки, временной ряд будет размерности $(t, K)$. И такой временной ряд можно подавать в модель на архитектуре transformer.